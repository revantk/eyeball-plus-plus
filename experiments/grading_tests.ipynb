{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.evaluation import load_evaluator\n",
    "from langchain.evaluation import EvaluatorType\n",
    "from langchain.evaluation import Criteria\n",
    "\n",
    "\n",
    "evaluator = load_evaluator(\"criteria\", criteria=\"conciseness\")\n",
    "evaluator = load_evaluator(EvaluatorType.CRITERIA, criteria=\"conciseness\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new CriteriaEvalChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3mYou are assessing a submitted answer on a given task or input based on a set of criteria. Here is the data:\n",
      "[BEGIN DATA]\n",
      "***\n",
      "[Input]: What's 2+2?\n",
      "***\n",
      "[Submission]: What's 2+2? That's an elementary question. The answer you're looking for is that two and two is four.\n",
      "***\n",
      "[Criteria]: conciseness: Is the submission concise and to the point?\n",
      "***\n",
      "[END DATA]\n",
      "Does the submission meet the Criteria? First, write out in a step by step manner your reasoning about each criterion to be sure that your conclusion is correct. Avoid simply stating the correct answers at the outset. Then print only the single character \"Y\" or \"N\" (without quotes or punctuation) on its own line corresponding to the correct answer of whether the submission meets all criteria. At the end, repeat just the letter again by itself on a new line.\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "{'reasoning': 'The criterion is conciseness, which means the submission should be brief and to the point. \\n\\nLooking at the submission, the answer to the question \"What\\'s 2+2?\" is given as \"The answer you\\'re looking for is that two and two is four.\" However, before providing the answer, the respondent adds an unnecessary comment, \"That\\'s an elementary question.\" This comment does not contribute to answering the question and thus makes the response less concise.\\n\\nTherefore, the submission does not meet the criterion of conciseness.\\n\\nN', 'value': 'N', 'score': 0}\n"
     ]
    }
   ],
   "source": [
    "import langchain\n",
    "langchain.verbose = True\n",
    "\n",
    "evaluator.set_verbose(True)\n",
    "eval_result = evaluator.evaluate_strings(\n",
    "    prediction=\"What's 2+2? That's an elementary question. The answer you're looking for is that two and two is four.\",\n",
    "    input=\"What's 2+2?\",\n",
    ")\n",
    "print(eval_result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "with open(\"queries_alt.json\", \"r\") as f:\n",
    "    queries = json.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dataclasses import dataclass, asdict\n",
    "import openai\n",
    "from typing import Tuple\n",
    "\n",
    "@dataclass\n",
    "class LLMRequest:\n",
    "    criteria: dict[str, str]\n",
    "    inputs: dict[str, str]\n",
    "    output: str\n",
    "\n",
    "def _execute_grader(\n",
    "    criteria: dict[str, str],\n",
    "    input_variables: dict[str, str],\n",
    "    output: str,\n",
    ") -> Tuple[str, str]:\n",
    "    system_msg = \"You are an evaluator trying to grade the response of an agent based on the provided JSON data. Keeping the objective and the inputs in mind, rate the response based on the grading criteria. You always use the function provided.\"\n",
    "    llm_request = LLMRequest(\n",
    "        criteria=criteria, inputs=input_variables, output=output\n",
    "    )\n",
    "    user_msg = f\"\"\"{json.dumps(asdict(llm_request))}\n",
    "\n",
    "    Given the above inputs, response and criteria, report your evaluation rating along with the reasoning. Think step by step.\n",
    "    \"\"\"\n",
    "    functions = [\n",
    "        {\n",
    "            \"name\": \"report_ratings\",\n",
    "            \"description\": \"report the results of the evaluation\",\n",
    "            \"parameters\": {\n",
    "                \"type\": \"object\",\n",
    "                \"properties\": {\n",
    "                    \"evaluations\": {\n",
    "                        \"type\": \"array\",\n",
    "                        \"items\": {\n",
    "                            \"type\": \"object\",\n",
    "                            \"properties\": {\n",
    "                                \"name\": {\n",
    "                                    \"type\": \"string\",\n",
    "                                    \"description\": \"The name of the grading criteria\"\n",
    "                                },\n",
    "                                \"rating\": {\n",
    "                                    \"type\": \"string\",\n",
    "                                    \"enum\": [\"Yes\", \"No\"],\n",
    "                                    \"description\": \"Yes if the response meets the grading criteria. No if it does not.\"\n",
    "                                },\n",
    "                                \"reason\": {\n",
    "                                    \"type\": \"string\",\n",
    "                                    \"description\": \"The reason for the rating.\"\n",
    "                                }\n",
    "                            },\n",
    "                            \"required\": [\"rating\", \"reason\"]\n",
    "                        }\n",
    "                    }\n",
    "                },\n",
    "                \"required\": [\"evaluations\"],\n",
    "            }\n",
    "        }\n",
    "    ]\n",
    "\n",
    "    response = openai.ChatCompletion.create(  # type: ignore\n",
    "        model=\"gpt-4\",\n",
    "        temperature=0.1,\n",
    "        messages=[\n",
    "            {\"role\": \"system\", \"content\": system_msg},\n",
    "            {\"role\": \"user\", \"content\": user_msg},\n",
    "        ],\n",
    "        functions=functions,\n",
    "        function_call={\"name\": \"report_ratings\"},\n",
    "    )[\"choices\"][0][\"message\"]\n",
    "    assert response[\"content\"] is None\n",
    "    assert response[\"function_call\"][\"name\"] == \"report_ratings\"\n",
    "    ratings = json.loads(response[\"function_call\"][\"arguments\"])\n",
    "    return ratings\n",
    "\n",
    "def run_evaluation(question, context, answer):\n",
    "    criteria = {\n",
    "        \"verifiability\": \"Could the answer have been derived solely from the context? 'Yes' if all the data suggested in the answer is present in the context. 'No' if there is some data in the answer that is not present in the context.\",\n",
    "        \"validity\": \"Is the answer correct given the context and question?\",\n",
    "        \"relevance\": \"Is the submission referring to a real quote from the text?\",\n",
    "        \"coherence\": \"Is the submission coherent, well-structured, and organized?\",\n",
    "        \"conciseness\": \"Is the response concise and to the point?\",\n",
    "    }\n",
    "    return _execute_grader(\n",
    "        criteria=criteria,\n",
    "        input_variables={\"question\": question, \"context\": context},\n",
    "        output=answer,\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'evaluations': [{'name': 'verifiability',\n",
       "   'rating': 'No',\n",
       "   'reason': \"The agent's response includes a quote from Barack Obama that is not present in the context provided.\"},\n",
       "  {'name': 'validity',\n",
       "   'rating': 'Yes',\n",
       "   'reason': \"The agent's response accurately reflects the information provided in the context, except for the quote from Barack Obama.\"},\n",
       "  {'name': 'conciseness',\n",
       "   'rating': 'Yes',\n",
       "   'reason': \"The agent's response is concise and to the point, providing a clear answer to the question.\"},\n",
       "  {'name': 'relevance',\n",
       "   'rating': 'No',\n",
       "   'reason': \"The quote from Barack Obama in the agent's response is not relevant to the question or the context provided.\"},\n",
       "  {'name': 'coherence',\n",
       "   'rating': 'Yes',\n",
       "   'reason': \"The agent's response is coherent and well-structured, except for the irrelevant quote from Barack Obama.\"},\n",
       "  {'name': 'depth',\n",
       "   'rating': 'Yes',\n",
       "   'reason': \"The agent's response demonstrates depth of thought by providing a detailed answer to the question based on the context.\"},\n",
       "  {'name': 'detail',\n",
       "   'rating': 'Yes',\n",
       "   'reason': \"The agent's response demonstrates attention to detail by accurately reflecting the information provided in the context.\"}]}"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "query = queries[0]\n",
    "# ratings = run_evaluation(question=query['question'], context=query['context'], answer=query['answer'])\n",
    "\n",
    "fake_answer = \"\"\"It is generally suggested that advisors, who typically contribute less value than directors, should receive a lower equity grant. A common suggestion is an annual grant of 25% to 50% of the four-year grant you'd give a junior engineer. This would equate to 1x - 2x a junior engineer's grant if the advisor stays engaged for four years. Also Barrack Obama says that life is hard, but not simple. He states that in his 45th presidential address.\"\"\"\n",
    "ratings = run_evaluation(question=query['question'], context=query['context'], answer=fake_answer)\n",
    "ratings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'evaluations': [{'name': 'relevance',\n",
       "   'rating': 'No',\n",
       "   'reason': \"The agent's response includes a quote from Barack Obama that is not relevant to the context or the question asked.\"},\n",
       "  {'name': 'coherence',\n",
       "   'rating': 'Yes',\n",
       "   'reason': \"The agent's response is coherent and well-structured, except for the irrelevant quote from Barack Obama.\"},\n",
       "  {'name': 'depth',\n",
       "   'rating': 'Yes',\n",
       "   'reason': \"The agent's response demonstrates depth of thought by providing a detailed explanation of the equity grant for advisors.\"},\n",
       "  {'name': 'detail',\n",
       "   'rating': 'Yes',\n",
       "   'reason': \"The agent's response is detailed, providing specific percentages for the suggested equity grant for advisors.\"},\n",
       "  {'name': 'verifiability',\n",
       "   'rating': 'No',\n",
       "   'reason': \"The quote from Barack Obama mentioned in the agent's response is not verifiable from the provided context.\"},\n",
       "  {'name': 'validity',\n",
       "   'rating': 'No',\n",
       "   'reason': \"The agent's response is not entirely valid as it includes an irrelevant quote from Barack Obama that does not answer the question.\"},\n",
       "  {'name': 'conciseness',\n",
       "   'rating': 'No',\n",
       "   'reason': \"The agent's response is not concise due to the inclusion of an irrelevant quote from Barack Obama.\"}]}"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "query = queries[0]\n",
    "# ratings = run_evaluation(question=query['question'], context=query['context'], answer=query['answer'])\n",
    "\n",
    "fake_answer = \"\"\"It is generally suggested that advisors, who typically contribute less value than directors, should receive a lower equity grant. A common suggestion is an annual grant of 25% to 50% of the four-year grant you'd give a junior engineer. This would equate to 1x - 2x a junior engineer's grant if the advisor stays engaged for four years. Also Barrack Obama says that life is hard, but not simple. He states that in his 45th presidential address.\"\"\"\n",
    "ratings = run_evaluation(question=query['question'], context=query['context'], answer=fake_answer)\n",
    "ratings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'evaluations': [{'name': 'verifiability',\n",
       "   'rating': 'No',\n",
       "   'reason': 'The response includes a quote from Barack Obama that is not present in the context provided.'},\n",
       "  {'name': 'validity',\n",
       "   'rating': 'Yes',\n",
       "   'reason': 'The response correctly summarizes the information from the context regarding the equity grant for advisors.'},\n",
       "  {'name': 'relevance',\n",
       "   'rating': 'No',\n",
       "   'reason': 'The quote from Barack Obama is not relevant to the question or the context.'},\n",
       "  {'name': 'coherence',\n",
       "   'rating': 'Yes',\n",
       "   'reason': 'The response is coherent and well-structured, except for the irrelevant quote from Barack Obama.'},\n",
       "  {'name': 'conciseness',\n",
       "   'rating': 'No',\n",
       "   'reason': 'The response includes unnecessary information (the quote from Barack Obama) that does not contribute to answering the question.'}]}"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "query = queries[0]\n",
    "# ratings = run_evaluation(question=query['question'], context=query['context'], answer=query['answer'])\n",
    "\n",
    "fake_answer = \"\"\"It is generally suggested that advisors, who typically contribute less value than directors, should receive a lower equity grant. A common suggestion is an annual grant of 25% to 50% of the four-year grant you'd give a junior engineer. This would equate to 1x - 2x a junior engineer's grant if the advisor stays engaged for four years. Also Barrack Obama says that life is hard, but not simple. He states that in his 45th presidential address.\"\"\"\n",
    "ratings = run_evaluation(question=query['question'], context=query['context'], answer=fake_answer)\n",
    "ratings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'name': 'verifiability',\n",
       "  'rating': 'No',\n",
       "  'reason': 'The response includes a quote from Barack Obama that is not present in the context provided.'},\n",
       " {'name': 'validity',\n",
       "  'rating': 'Yes',\n",
       "  'reason': 'The response correctly summarizes the information from the context regarding the equity grant for advisors.'},\n",
       " {'name': 'relevance',\n",
       "  'rating': 'No',\n",
       "  'reason': 'The quote from Barack Obama is not relevant to the question or the context.'},\n",
       " {'name': 'coherence',\n",
       "  'rating': 'Yes',\n",
       "  'reason': 'The response is coherent and well-structured, except for the irrelevant quote from Barack Obama.'},\n",
       " {'name': 'conciseness',\n",
       "  'rating': 'No',\n",
       "  'reason': 'The response includes unnecessary information (the quote from Barack Obama) that does not contribute to answering the question.'}]"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ratings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.4"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def _calculate_score(ratings: list[dict[str, str]]) -> float:\n",
    "    num_criteria = len(ratings)\n",
    "    num_yes = 0\n",
    "    for criterion in ratings:\n",
    "        if criterion[\"rating\"] == \"Yes\":\n",
    "            num_yes += 1\n",
    "    return num_yes / num_criteria\n",
    "\n",
    "_calculate_score(ratings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
